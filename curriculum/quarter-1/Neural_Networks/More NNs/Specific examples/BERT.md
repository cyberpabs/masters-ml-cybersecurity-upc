Short for **Bidirectional Encoder Representations from Transformers**, is a a pre-trained
language model developed by Google in 2018 for [[Natural Language Processing]].

 Is trained on massive datasets (like Wikipedia and BookCorpus) using
**unsupervised** methods. It learns general-purpose language representations.

![[BERT_example_1.png]]