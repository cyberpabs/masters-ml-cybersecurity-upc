ReLU is one of the most popular activation functions for artificial [[Neural Networks]], and finds application in [[computer vision]] and [[speech recognition]] using [[deep neural networks]] and computational neuroscience.

It's an activation function defined as the non-negative part of its argument, i.e., the ramp function:
![[rectified_linear_function_example.png]]
And the formula equals:
$$
a = max(0, z)
$$
